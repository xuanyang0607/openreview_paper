# openreview_paper
The python code for Statistical Analysis of Reviewer Quality in Auditing. This repo provides novel experimentation and analysis to help evaluate reviewer agreement rating using Fleiss Kappa and hypothesis testing to help auditing evaluate reviewer agreement rate as well as review quality given auditors' for a product area

This is a synthetic data set consisting of three reviewers and their answers regarding a nine question rubric on 1528 products. The goal is to evaluate if there are certain rubric questions that are subject to bias as well as if reviewers will be more prone to be biased by certain questions, and overall what is the agreement rate among reviewers and as a whole. 
